from __future__ import annotations
from abc import abstractmethod
from dataclasses import dataclass
from enum import Enum


class TokenType(Enum):
    """
    TokenType is an enumeration of all the possible token types that can be generated by the lexer. Override this class
    to add other tokens, and implement the single_line_comment_token and multi_line_comment_token methods to specify the
    token types for single line comments and multi line comments.

    Standard prefixes that MUST be followed:
        - Kw: Keyword
        - Lx: Lexeme (assign a regex pattern to the value of the token)
        - Tk: Token

    Examples:
        - KwIf = "if"
        - LxNumber = r"[0-9]+"
        - TkPlus = "+"

    Look at the Lexer class to see how to register custom token sets.
    """

    ERR = "Unknown"
    NO_TOK = ""
    EOF = "<EOF>"
    NEWLINE = "\n"
    WHITE_SPACE = " "

    @staticmethod
    @abstractmethod
    def single_line_comment_token() -> TokenType:
        return TokenType.ERR

    @staticmethod
    @abstractmethod
    def multi_line_comment_token() -> TokenType:
        return TokenType.ERR


@dataclass
class Token:
    """
    Token is a dataclass that represents a token generated by the lexer. It contains the metadata of the token and the
    type of the token. The metadata is the actual text that the token represents. For regex based tokens, the metadata
    will be the actual match of the regex.

    There is no need to override this class, as it is a simple dataclass that holds the token metadata and the token
    type.
    """

    token_metadata: str
    token_type: TokenType

    def __str__(self):
        return self.token_metadata
